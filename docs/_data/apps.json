[{"name": "CLAMS wrapper for spaCy NLP", "description": "Apply spaCy NLP to all text documents in a MMIF file.", "app_version": "v1", "mmif_version": "0.5.0", "analyzer_version": "3.1.2", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/spacy-wrapper/v1", "url": "https://github.com/clamsproject/app-spacy-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "required": true}, {"@type": "http://vocab.lappsgrid.org/Token", "required": false}], "output": [{"@type": "http://vocab.lappsgrid.org/Token"}, {"@type": "http://vocab.lappsgrid.org/Token#pos"}, {"@type": "http://vocab.lappsgrid.org/Token#lemma"}, {"@type": "http://vocab.lappsgrid.org/NounChunk"}, {"@type": "http://vocab.lappsgrid.org/Sentence"}, {"@type": "http://vocab.lappsgrid.org/NamedEntity"}], "parameters": [{"name": "pretokenized", "description": "Boolean parameter to set the app to use existing tokenization, if available, for text documents for NLP processing. Useful to process ASR documents, for example.", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Whisper Wrapper", "description": "A CLAMS wrapper for Whisper-based ASR software originally developed by OpenAI.", "app_version": "v1", "mmif_version": "1.0.0", "analyzer_version": "20230314", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/whisper-wrapper/v1", "url": "https://github.com/clamsproject/app-whisper-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeUnit": "seconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}]}, {"name": "AAPB-PUA Kaldi Wrapper", "description": "A CLAMS wrapper for Kaldi-based ASR software originally developed by PopUpArchive and hipstas, and later updated by Kyeongmin Rim at Brandeis University. Wrapped software can be found at https://github.com/brandeis-llc/aapb-pua-kaldi-docker . ", "app_version": "v1", "mmif_version": "0.5.0", "analyzer_version": "v4", "app_license": "Apache 2.0", "analyzer_license": "UNKNOWN", "identifier": "http://apps.clams.ai/aapb-pua-kaldi-wrapper/v1", "url": "https://github.com/clamsproject/app-aapb-pua-kaldi-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}], "parameters": [{"name": "use_speech_segmentation", "description": "When true, the app looks for existing TimeFrame { \"frameType\": \"speech\" } annotations, and runs ASR only on those frames, instead of entire audio files.", "type": "boolean", "default": true, "multivalued": false}]}, {"name": "Whisper Wrapper", "description": "A CLAMS wrapper for Whisper-based ASR software originally developed by OpenAI.", "app_version": "v2", "mmif_version": "1.0.0", "analyzer_version": "20230314", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/whisper-wrapper/v2", "url": "https://github.com/clamsproject/app-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeUnit": "seconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}]}, {"name": "Brandeis ACS Wrapper", "description": "Brandeis Acoustic Classification & Segmentation (ACS) is a audio segmentation tool developed at Brandeis Lab for Linguistics and Computation. The original software can be found at https://github.com/brandeis-llc/acoustic-classification-segmentation .", "app_version": "v1", "mmif_version": "1.0.0", "analyzer_version": "1.11", "app_license": "Apache2.0", "analyzer_license": "Apache2.0", "identifier": "http://apps.clams.ai/brandeis-acs-wrapper/v1", "url": "https://github.com/clamsproject/app-brandeis-acs-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeunit": "milliseconds"}}]}, {"name": "AAPB-PUA Kaldi Wrapper", "description": "A CLAMS wrapper for Kaldi-based ASR software originally developed by PopUpArchive and hipstas, and later updated by Kyeongmin Rim at Brandeis University. Wrapped software can be found at https://github.com/brandeis-llc/aapb-pua-kaldi-docker . ", "app_version": "v2", "mmif_version": "1.0.0", "analyzer_version": "v4", "app_license": "Apache 2.0", "analyzer_license": "UNKNOWN", "identifier": "http://apps.clams.ai/aapb-pua-kaldi-wrapper/v2", "url": "https://github.com/clamsproject/app-aapb-pua-kaldi-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}], "parameters": [{"name": "use_speech_segmentation", "description": "When true, the app looks for existing TimeFrame { \"frameType\": \"speech\" } annotations, and runs ASR only on those frames, instead of entire audio files.", "type": "boolean", "default": true, "multivalued": false}]}, {"name": "Slate Detection", "description": "This tool detects slates.", "app_version": "v1.0", "mmif_version": "1.0.0", "app_license": "MIT", "identifier": "http://apps.clams.ai/slatedetection/v1.0", "url": "https://github.com/clams-project/app-slatedetection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"properties": {"frameType": "string"}}}], "parameters": [{"name": "timeUnit", "description": "Unit for output typeframe", "type": "string", "choices": ["frames", "milliseconds"], "default": "frames", "multivalued": false}, {"name": "sampleRatio", "description": "Frequency to sample frames.", "type": "integer", "default": 30, "multivalued": false}, {"name": "stopAt", "description": "Frame number to stop processing", "type": "integer", "default": 540000, "multivalued": false}, {"name": "stopAfterOne", "description": "When True, processing stops after first timeframe is found", "type": "boolean", "default": 1, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of frames required for a timeframe to be included in the output", "type": "integer", "default": 10, "multivalued": false}, {"name": "threshold", "description": "Threshold from 0-1, lower accepts more potential slates.", "type": "number", "default": 0, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Pyscenedetect Wrapper", "description": "", "app_version": "v1", "mmif_version": "1.0.0", "analyzer_version": "0.6.1", "app_license": "Apache2", "analyzer_license": "BSD-3", "identifier": "http://apps.clams.ai/pyscenedetect-wrapper/v1", "url": "https://github.com/clamsproject/app-pyscenedetect-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"frameType": "shot", "timeUnit": "frame"}}], "parameters": [{"name": "mode", "description": "pick a scene detector algorithm, see http://scenedetect.com/projects/Manual/en/latest/cli/detectors.html", "type": "string", "choices": ["content", "threshold", "adaptive"], "default": "content", "multivalued": false}, {"name": "threshold", "description": "threshold value to use in the detection algorithm. Note that the meaning of this numerical value differs for different detector algorithms.", "type": "number", "default": 27, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Slate Detection", "description": "This tool detects slates.", "app_version": "v1.1", "mmif_version": "1.0.0", "app_license": "MIT", "identifier": "http://apps.clams.ai/slatedetection/v1.1", "url": "https://github.com/clamsproject/app-slatedetection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"properties": {"frameType": "string"}}}], "parameters": [{"name": "timeUnit", "description": "Unit for output typeframe", "type": "string", "choices": ["frames", "milliseconds"], "default": "frames", "multivalued": false}, {"name": "sampleRatio", "description": "Frequency to sample frames.", "type": "integer", "default": 30, "multivalued": false}, {"name": "stopAt", "description": "Frame number to stop processing", "type": "integer", "default": 540000, "multivalued": false}, {"name": "stopAfterOne", "description": "When True, processing stops after first timeframe is found", "type": "boolean", "default": 1, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of frames required for a timeframe to be included in the output", "type": "integer", "default": 10, "multivalued": false}, {"name": "threshold", "description": "Threshold from 0-1, lower accepts more potential slates.", "type": "number", "default": 0, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Slate Detection", "description": "This tool detects slates.", "app_version": "v1.2", "mmif_version": "1.0.0", "app_license": "MIT", "identifier": "http://apps.clams.ai/slatedetection/v1.2", "url": "https://github.com/clamsproject/app-slatedetection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"properties": {"frameType": "string"}}}], "parameters": [{"name": "timeUnit", "description": "Unit for output typeframe", "type": "string", "choices": ["frames", "milliseconds"], "default": "frames", "multivalued": false}, {"name": "sampleRatio", "description": "Frequency to sample frames.", "type": "integer", "default": 30, "multivalued": false}, {"name": "stopAt", "description": "Frame number to stop processing", "type": "integer", "default": 540000, "multivalued": false}, {"name": "stopAfterOne", "description": "When True, processing stops after first timeframe is found", "type": "boolean", "default": 1, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of frames required for a timeframe to be included in the output", "type": "integer", "default": 10, "multivalued": false}, {"name": "threshold", "description": "Threshold from 0-1, lower accepts more potential slates.", "type": "number", "default": 0, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "inaSpeechSegmenter Wrapper", "description": "inaSpeechSegmenter is a CNN-based audio segmentation toolkit. The original software can be found at https://github.com/ina-foss/inaSpeechSegmenter .", "app_version": "v1.0", "mmif_version": "1.0.0", "analyzer_version": "0.7.6", "app_license": "MIT", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/inaspeechsegmenter-wrapper/v1.0", "url": "https://github.com/clamsproject/app-inaspeechsegmenter-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeunit": "milliseconds"}}], "parameters": [{"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Slate Detection", "description": "This tool detects slates.", "app_version": "v2.0", "mmif_version": "1.0.0", "app_license": "MIT", "identifier": "http://apps.clams.ai/slatedetection/v2.0", "url": "https://github.com/clamsproject/app-slatedetection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"properties": {"frameType": "slate"}}}], "parameters": [{"name": "timeUnit", "description": "Unit of time to use in output.", "type": "string", "choices": ["frames", "seconds", "milliseconds"], "default": "frames", "multivalued": false}, {"name": "sampleRatio", "description": "Frequency to sample frames.", "type": "integer", "default": 30, "multivalued": false}, {"name": "stopAt", "description": "Frame number to stop processing", "type": "integer", "default": 9000, "multivalued": false}, {"name": "stopAfterOne", "description": "When True, processing stops after first timeframe is found", "type": "boolean", "default": true, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of frames required for a timeframe to be included in the output", "type": "integer", "default": 10, "multivalued": false}, {"name": "threshold", "description": "Threshold from 0-1, lower accepts more potential slates.", "type": "number", "default": 0.7, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "inaSpeechSegmenter Wrapper", "description": "inaSpeechSegmenter is a CNN-based audio segmentation toolkit. The original software can be found at https://github.com/ina-foss/inaSpeechSegmenter .", "app_version": "v1.1", "mmif_version": "1.0.0", "analyzer_version": "0.7.6", "app_license": "MIT", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/inaspeechsegmenter-wrapper/v1.1", "url": "https://github.com/clamsproject/app-inaspeechsegmenter-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeunit": "milliseconds"}}], "parameters": [{"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Brandeis ACS Wrapper", "description": "Brandeis Acoustic Classification & Segmentation (ACS) is a audio segmentation tool developed at Brandeis Lab for Linguistics and Computation. The original software can be found at https://github.com/brandeis-llc/acoustic-classification-segmentation .", "app_version": "v2", "mmif_version": "1.0.0", "analyzer_version": "1.11", "app_license": "Apache2.0", "analyzer_license": "Apache2.0", "identifier": "http://apps.clams.ai/brandeis-acs-wrapper/v2", "url": "https://github.com/clamsproject/app-brandeis-acs-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeunit": "milliseconds"}}], "parameters": [{"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Whisper Wrapper", "description": "A CLAMS wrapper for Whisper-based ASR software originally developed by OpenAI.", "app_version": "v3", "mmif_version": "1.0.0", "analyzer_version": "20230314", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/whisper-wrapper/v3", "url": "https://github.com/clamsproject/app-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeUnit": "seconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}], "parameters": [{"name": "modelSize", "description": "The size of the model to use. Can be \"tiny\", \"base\", \"small\", \"medium\", or \"large\".", "type": "string", "choices": ["tiny", "base", "small", "medium", "large"], "default": "tiny", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Pyscenedetect Wrapper", "description": "", "app_version": "v2", "mmif_version": "1.0.0", "analyzer_version": "0.6.1", "app_license": "Apache2", "analyzer_license": "BSD-3", "identifier": "http://apps.clams.ai/pyscenedetect-wrapper/v2", "url": "https://github.com/clamsproject/app-pyscenedetect-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"frameType": "shot", "timeUnit": "frame"}}], "parameters": [{"name": "mode", "description": "pick a scene detector algorithm, see http://scenedetect.com/projects/Manual/en/latest/cli/detectors.html", "type": "string", "choices": ["content", "threshold", "adaptive"], "default": "content", "multivalued": false}, {"name": "threshold", "description": "threshold value to use in the detection algorithm. Note that the meaning of this numerical value differs for different detector algorithms.", "type": "number", "default": 27, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "EAST Text Detection", "description": "OpenCV-based text localization app that used EAST text detection model. Please visit the source code repository for full documentation.", "app_version": "v1.0", "mmif_version": "1.0.0", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/east-textdetection/v1.0", "url": "https://github.com/clamsproject/app-east-textdetection", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/ImageDocument/v1", "required": true}], {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "required": false}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/BoundingBox/v1", "properties": {"bboxtype": "text"}}], "parameters": [{"name": "timeUnit", "description": "Unit for time points in the output. Only works with VideoDocument input.", "type": "string", "choices": ["frames", "seconds", "milliseconds"], "default": "frames", "multivalued": false}, {"name": "frameType", "description": "Segments of video to run on. Only works with VideoDocument input and TimeFrame input. Empty value means run on the every frame types.", "type": "string", "choices": ["", "slate", "chyron", "rolling-credit"], "default": "", "multivalued": true}, {"name": "sampleRatio", "description": "Frequency to sample frames. Only works with VideoDocument input, and without TimeFrame input. (when `TimeFrame` annotation is found, this parameter is ignored.)", "type": "integer", "default": 30, "multivalued": false}, {"name": "stopAt", "description": "Frame number to stop running. Only works with VideoDocument input. The default is roughly 2 hours of video at 30fps.", "type": "integer", "default": "2 * 60 * 60 * 30", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Dbpedia Spotlight Wrapper", "description": "Apply named entity linking to all text documents in a MMIF file.", "app_version": "v1.0", "mmif_version": "1.0.0", "analyzer_version": "version_1.0", "app_license": "Apache 2.0", "analyzer_license": "Apache 2.0", "identifier": "http://apps.clams.ai/dbpedia-spotlight-wrapper/v1.0", "url": "https://github.com/clamsproject/app-dbpedia-spotlight-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "required": true}], "output": [{"@type": "http://vocab.lappsgrid.org/NamedEntity"}], "parameters": [{"name": "confidence", "description": "disambiguation confidence score for linking", "type": "number", "default": 0.5, "multivalued": false}, {"name": "support", "description": "resource prominence, i.e. number of in-links in Wikipedia (lower bound)", "type": "integer", "default": 0, "multivalued": false}, {"name": "types", "description": "limits recognition to certain types of named entities, e.g. DBpedia:Place", "type": "string", "multivalued": true}, {"name": "policy", "description": "(whitelist) selects all entities of the same type; (blacklist) selects all entities not of the same type", "type": "string", "choices": ["whitelist", "blacklist"], "default": "whitelist", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "CLAMS wrapper for spaCy NLP", "description": "Apply spaCy NLP to all text documents in a MMIF file.", "app_version": "v1.1", "mmif_version": "1.0.0", "analyzer_version": "3.6", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/spacy-wrapper/v1.1", "url": "https://github.com/clamsproject/app-spacy-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "required": true}, {"@type": "http://vocab.lappsgrid.org/Token", "required": false}], "output": [{"@type": "http://vocab.lappsgrid.org/Token"}, {"@type": "http://vocab.lappsgrid.org/Token#pos"}, {"@type": "http://vocab.lappsgrid.org/Token#lemma"}, {"@type": "http://vocab.lappsgrid.org/NounChunk"}, {"@type": "http://vocab.lappsgrid.org/Sentence"}, {"@type": "http://vocab.lappsgrid.org/NamedEntity"}], "parameters": [{"name": "pretokenized", "description": "Boolean parameter to set the app to use existing tokenization, if available, for text documents for NLP processing. Useful to process ASR documents, for example.", "type": "boolean", "default": 0, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Tone_Detector", "description": "Detects spans of monotonic audio within an audio file", "app_version": "v1.0", "mmif_version": "1.0.0", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/tonedetection/v1.0", "url": "https://github.com/clamsproject/app-tonedetection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"frameType": "tone"}}], "parameters": [{"name": "timeUnit", "description": "the unit for annotation output", "type": "string", "choices": ["seconds", "seconds", "milliseconds"], "default": "seconds", "multivalued": false}, {"name": "lengthThreshold", "description": "minimum length threshold (in ms)", "type": "integer", "default": 2000, "multivalued": false}, {"name": "sampleSize", "description": "length for each segment of samples to be compared", "type": "integer", "default": 512, "multivalued": false}, {"name": "stopAt", "description": "stop point for audio processing (in ms). Defaults to the length of the file", "type": "integer", "default": "None", "multivalued": false}, {"name": "tolerance", "description": "threshold value for a \"match\" within audio processing", "type": "number", "default": 1, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Gentle Forced Aligner Wrapper", "description": "This CLAMS app aligns transcript and audio track using Gentle. Gentle is a robust yet lenient forced aligner built on Kaldi.This app only works when Gentle is already installed locally.Unfortunately, Gentle is not distributed as a Python package distribution.To get Gentle installation instruction, see https://lowerquality.com/gentle/ Make sure install Gentle from the git commit specified in ``analyzer_version`` in this metadata.", "app_version": "v1.0", "mmif_version": "1.0.0", "analyzer_version": "f29245a", "app_license": "MIT", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/gentle-forced-aligner-wrapper/v1.0", "url": "https://github.com/clamsproject/app-gentle-forced-aligner-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"frameType": "speech"}, "required": false}, {"@type": "http://vocab.lappsgrid.org/Token", "required": false}], "output": [{"@type": "http://vocab.lappsgrid.org/Token"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"frameType": "speech", "timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}], "parameters": [{"name": "use_speech_segmentation", "description": "When set true, use exising \"speech\"-typed ``TimeFrame`` annotations and run aligner only on those frames, instead of entire audio files.", "type": "boolean", "default": true, "multivalued": false}, {"name": "use_tokenization", "description": "When set true, ``Alignment`` annotation output will honor existing latest tokenization (``Token`` annotations). Due to a limitation of the way Kaldi reads in English tokens, existing tokens must not contain whitespaces. ", "type": "boolean", "default": true, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Chyron Detection", "description": "This tool detects chyrons, generates time segments.", "app_version": "v1.0", "mmif_version": "1.0.0", "app_license": "MIT", "identifier": "http://apps.clams.ai/chyron-detection/v1.0", "url": "https://github.com/clamsproject/app-chyron-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"properties": {"frameType": "chyron"}}}], "parameters": [{"name": "timeUnit", "description": "unit for output timeframe", "type": "string", "choices": ["frames", "seconds", "milliseconds"], "default": "frames", "multivalued": false}, {"name": "sampleRatio", "description": "Frequency to sample frames", "type": "integer", "default": 5, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of frames required for a timeframe to be included", "type": "integer", "default": 10, "multivalued": false}, {"name": "threshold", "description": "Threshold from 0-1, lower accepts more potential chyrons", "type": "number", "default": 0.5, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Dbpedia Spotlight Wrapper", "description": "Apply named entity linking to all text documents in a MMIF file.", "app_version": "v1.1", "mmif_version": "1.0.0", "analyzer_version": "daf5309", "app_license": "Apache 2.0", "analyzer_license": "Apache 2.0", "identifier": "http://apps.clams.ai/dbpedia-spotlight-wrapper/v1.1", "url": "https://github.com/clamsproject/app-dbpedia-spotlight-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "required": true}], "output": [{"@type": "http://vocab.lappsgrid.org/NamedEntity"}], "parameters": [{"name": "confidence", "description": "disambiguation confidence score for linking", "type": "number", "default": 0.5, "multivalued": false}, {"name": "support", "description": "resource prominence, i.e. number of in-links in Wikipedia (lower bound)", "type": "integer", "default": 0, "multivalued": false}, {"name": "types", "description": "types filter", "type": "string", "multivalued": false}, {"name": "policy", "description": "(whitelist) selects all entities of the same type; (blacklist) selects all entities not of the same type", "type": "string", "choices": ["whitelist", "blacklist"], "default": "whitelist", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Tesseract OCR Wrapper", "description": "This tool applies Tesseract OCR to a video or image and generates text boxes and OCR results.", "app_version": "v1.0", "mmif_version": "1.0.0", "analyzer_version": "tesseract4", "app_license": "MIT", "analyzer_license": "apache", "identifier": "http://apps.clams.ai/tesseractocr-wrapper/v1.0", "url": "https://github.com/clamsproject/app-tesseractocr-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/BoundingBox/v1", "properties": {"boxType": "text"}, "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "required": false}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}], "parameters": [{"name": "frameType", "description": "Use this to specify TimeFrame to use for filtering \"text\"-typed BoundingBox annotations. Can be \"slate\", \"chyron\", \"speech\", etc.. If not set, the app won't use TimeFrames for filtering.", "type": "string", "default": "", "multivalued": true}, {"name": "threshold", "description": "Use this value between 0 and 1 to filter out low-confidence text boxes.", "type": "number", "default": 0.9, "multivalued": false}, {"name": "psm", "description": "Tesseract Page Segmentation Modes. See https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html#page-segmentation-method", "type": "integer", "choices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "default": 0, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Bars Detection", "description": "This tool detects SMPTE color bars.", "app_version": "v1.0", "mmif_version": "1.0.0", "app_license": "MIT", "identifier": "http://apps.clams.ai/barsdetection/v1.0", "url": "https://github.com/clamsproject/app-barsdetection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"typeSpecificProperty": {"frameType": "bars"}}}], "parameters": [{"name": "timeUnit", "description": "Unit for output typeframe.", "type": "string", "choices": ["frames", "seconds", "milliseconds"], "default": "frames", "multivalued": false}, {"name": "sampleRatio", "description": "Frequency to sample frames.", "type": "integer", "default": 30, "multivalued": false}, {"name": "stopAt", "description": "Frame number to stop processing.", "type": "integer", "default": 9000, "multivalued": false}, {"name": "stopAfterOne", "description": "When True, processing stops after first timeframe is found.", "type": "boolean", "default": true, "multivalued": false}, {"name": "minFrameCount", "description": "minimum number of frames required for a timeframe to be included in the output.", "type": "integer", "default": 10, "multivalued": false}, {"name": "threshold", "description": "Threshold from 0-1, lower accepts more potential slates.", "type": "number", "default": 0.7, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Parseq OCR Wrapper", "description": "This tool applies Parseq OCR to a video or image and generates text boxes and OCR results.", "app_version": "v1.0", "mmif_version": "1.0.0", "analyzer_version": "bc8d95cd", "app_license": "MIT", "analyzer_license": "Apache 2.0", "identifier": "http://apps.clams.ai/parseqocr-wrapper/v1.0", "url": "https://github.com/clamsproject/app-parseqocr-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/BoundingBox/v1", "properties": {"boxType": "text"}, "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}], "parameters": [{"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "EAST Text Detection", "description": "OpenCV-based text localization app that used EAST text detection model. Please visit the source code repository for full documentation.", "app_version": "v1.1", "mmif_version": "1.0.0", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/east-textdetection/v1.1", "url": "https://github.com/clamsproject/app-east-textdetection", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/ImageDocument/v1", "required": true}], {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "required": false}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/BoundingBox/v1", "properties": {"bboxtype": "text"}}], "parameters": [{"name": "timeUnit", "description": "Unit for time points in the output. Only works with VideoDocument input.", "type": "string", "choices": ["frames", "seconds", "milliseconds"], "default": "frames", "multivalued": false}, {"name": "frameType", "description": "Segments of video to run on. Only works with VideoDocument input and TimeFrame input. Empty value means run on the every frame types.", "type": "string", "choices": ["", "slate", "chyron", "rolling-credit"], "default": "", "multivalued": true}, {"name": "sampleRatio", "description": "Frequency to sample frames. Only works with VideoDocument input, and without TimeFrame input. (when `TimeFrame` annotation is found, this parameter is ignored.)", "type": "integer", "default": 30, "multivalued": false}, {"name": "stopAt", "description": "Frame number to stop running. Only works with VideoDocument input. The default is roughly 2 hours of video at 30fps.", "type": "integer", "default": "2 * 60 * 60 * 30", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Bars Detection", "description": "This tool detects SMPTE color bars.", "app_version": "v1.1", "mmif_version": "1.0.0", "app_license": "MIT", "identifier": "http://apps.clams.ai/barsdetection/v1.1", "url": "https://github.com/clamsproject/app-barsdetection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"typeSpecificProperty": {"frameType": "bars"}}}], "parameters": [{"name": "timeUnit", "description": "Unit for output typeframe.", "type": "string", "choices": ["frames", "seconds", "milliseconds"], "default": "frames", "multivalued": false}, {"name": "sampleRatio", "description": "Frequency to sample frames.", "type": "integer", "default": 30, "multivalued": false}, {"name": "stopAt", "description": "Frame number to stop processing.", "type": "integer", "default": 9000, "multivalued": false}, {"name": "stopAfterOne", "description": "When True, processing stops after first timeframe is found.", "type": "boolean", "default": true, "multivalued": false}, {"name": "minFrameCount", "description": "minimum number of frames required for a timeframe to be included in the output.", "type": "integer", "default": 10, "multivalued": false}, {"name": "threshold", "description": "Threshold from 0-1, lower accepts more potential slates.", "type": "number", "default": 0.7, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Few Shot Classifier", "description": "This tool uses a vision model to classify video segments. Currenly supports \"chyron\" frame type.", "app_version": "v1.0", "mmif_version": "1.0.0", "analyzer_version": "1.0", "app_license": "MIT", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/fewshotclassifier/v1.0", "url": "https://github.com/clamsproject/app-fewshotclassifier", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"frameType": "string"}}], "parameters": [{"name": "timeUnit", "description": "Unit for output timeframe", "type": "string", "choices": ["frames", "milliseconds"], "default": "frames", "multivalued": false}, {"name": "sampleRatio", "description": "Frequency to sample frames.", "type": "integer", "default": 30, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of frames required for a timeframe to be included in the output with a minimum value of 1", "type": "integer", "default": 60, "multivalued": false}, {"name": "threshold", "description": "Threshold from 0-1, lower accepts more potential labels.", "type": "number", "default": 0.8, "multivalued": false}, {"name": "finetunedFrameType", "description": "Name of fine-tuned model to use. All pre-installed models are named after the frame type they were fine-tuned for.\n\nIf an empty value is passed, the app will look for fewshots.csv file in the same directory as the app.py and create a new fine-tuned model based on the examples in that file.\n\nAt the moment, a model fine-tuned on \"chyron\" frame type is shipped as pre-installed.", "type": "string", "default": "chyron", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Slate Detection", "description": "This tool detects slates.", "app_version": "v2.1", "mmif_version": "1.0.0", "app_license": "MIT", "identifier": "http://apps.clams.ai/slatedetection/v2.1", "url": "https://github.com/clamsproject/app-slatedetection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"properties": {"frameType": "slate"}}}], "parameters": [{"name": "timeUnit", "description": "Unit of time to use in output.", "type": "string", "choices": ["frames", "seconds", "milliseconds"], "default": "frames", "multivalued": false}, {"name": "sampleRatio", "description": "Frequency to sample frames.", "type": "integer", "default": 30, "multivalued": false}, {"name": "stopAt", "description": "Frame number to stop processing", "type": "integer", "default": 9000, "multivalued": false}, {"name": "stopAfterOne", "description": "When True, processing stops after first timeframe is found", "type": "boolean", "default": true, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of frames required for a timeframe to be included in the output", "type": "integer", "default": 10, "multivalued": false}, {"name": "threshold", "description": "Threshold from 0-1, lower accepts more potential slates.", "type": "number", "default": 0.7, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Dbpedia Spotlight Wrapper", "description": "Apply named entity linking to all text documents in a MMIF file.", "app_version": "v1.2", "mmif_version": "1.0.0", "analyzer_version": "daf5309", "app_license": "Apache 2.0", "analyzer_license": "Apache 2.0", "identifier": "http://apps.clams.ai/dbpedia-spotlight-wrapper/v1.2", "url": "https://github.com/clamsproject/app-dbpedia-spotlight-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "required": true}], "output": [{"@type": "http://vocab.lappsgrid.org/NamedEntity"}], "parameters": [{"name": "confidence", "description": "disambiguation confidence score for linking", "type": "number", "default": 0.5, "multivalued": false}, {"name": "support", "description": "resource prominence, i.e. number of in-links in Wikipedia (lower bound)", "type": "integer", "default": 0, "multivalued": false}, {"name": "types", "description": "limits recognition to certain types of named entities, e.g. DBpedia:Place", "type": "string", "multivalued": true}, {"name": "policy", "description": "(whitelist) selects all entities of the same type; (blacklist) selects all entities not of the same type", "type": "string", "choices": ["whitelist", "blacklist"], "default": "whitelist", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Scene-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v1.0", "mmif_version": "1.0.0", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v1.0", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1"}], "parameters": [{"name": "model", "description": "the model to use, not implemented yet", "type": "string", "default": "vgg16", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v2.0", "mmif_version": "1.0.0", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v2.0", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1"}], "parameters": [{"name": "sampleRate", "description": "Milliseconds between sampled frames", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minFrameScore", "description": "Minimum score for a still frame to be included in a TimeFrame", "type": "number", "default": 0.01, "multivalued": false}, {"name": "minTimeframeScore", "description": "Minimum score for a TimeFrame", "type": "number", "default": 0.25, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of sampled frames required for a TimeFrame", "type": "integer", "default": 2, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Whisper Wrapper", "description": "A CLAMS wrapper for Whisper-based ASR software originally developed by OpenAI.", "app_version": "v4", "mmif_version": "1.0.0", "analyzer_version": "20231117", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/whisper-wrapper/v4", "url": "https://github.com/clamsproject/app-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeUnit": "seconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}], "parameters": [{"name": "modelSize", "description": "The size of the model to use. Can be \"tiny\", \"base\", \"small\", \"medium\", or \"large\".", "type": "string", "choices": ["tiny", "base", "small", "medium", "large"], "default": "tiny", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Easyocr Wrapper", "description": "Using EasyOCR to extract text from timeframes", "app_version": "v1.0", "mmif_version": "1.0.0", "analyzer_version": "1.7.0", "app_license": "MIT", "analyzer_license": "Apache 2.0", "identifier": "http://apps.clams.ai/easyocr-wrapper/v1.0", "url": "https://github.com/clamsproject/app-easyocr-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/BoundingBox/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v1"}], "parameters": [{"name": "sampleFrames", "description": "Number of frames to sample from timeframe", "type": "integer", "default": 1, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v2.1", "mmif_version": "1.0.0", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v2.1", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeUnit": "milliseconds"}}], "parameters": [{"name": "sampleRate", "description": "Milliseconds between sampled frames", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minFrameScore", "description": "Minimum score for a still frame to be included in a TimeFrame", "type": "number", "default": 0.01, "multivalued": false}, {"name": "minTimeframeScore", "description": "Minimum score for a TimeFrame", "type": "number", "default": 0.25, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of sampled frames required for a TimeFrame", "type": "integer", "default": 2, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Easyocr Wrapper", "description": "Using EasyOCR to extract text from timeframes", "app_version": "v1.1", "mmif_version": "1.0.0", "analyzer_version": "1.7.0", "app_license": "MIT", "analyzer_license": "Apache 2.0", "identifier": "http://apps.clams.ai/easyocr-wrapper/v1.1", "url": "https://github.com/clamsproject/app-easyocr-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/BoundingBox/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v1"}], "parameters": [{"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v3.0", "mmif_version": "1.0.0", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v3.0", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeUnit": "milliseconds", "frameType": "bars"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeUnit": "milliseconds", "frameType": "slate"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeUnit": "milliseconds", "frameType": "chyron"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeUnit": "milliseconds", "frameType": "credits"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v1", "properties": {"timeUnit": "milliseconds"}}], "parameters": [{"name": "startAt", "description": "Number of milliseconds into the video to start processing", "type": "integer", "default": 0, "multivalued": false}, {"name": "stopAt", "description": "Number of milliseconds into the video to stop processing", "type": "integer", "default": 10000000, "multivalued": false}, {"name": "sampleRate", "description": "Milliseconds between sampled frames", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minFrameScore", "description": "Minimum score for a still frame to be included in a TimeFrame", "type": "number", "default": 0.01, "multivalued": false}, {"name": "minTimeframeScore", "description": "Minimum score for a TimeFrame", "type": "number", "default": 0.25, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of sampled frames required for a TimeFrame", "type": "integer", "default": 2, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Whisper Wrapper", "description": "A CLAMS wrapper for Whisper-based ASR software originally developed by OpenAI.", "app_version": "v5", "mmif_version": "1.0.0", "analyzer_version": "20231117", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/whisper-wrapper/v5", "url": "https://github.com/clamsproject/app-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v1", "properties": {"timeUnit": "millisecond"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}, {"@type": "http://vocab.lappsgrid.org/Sentence"}], "parameters": [{"name": "modelSize", "description": "The size of the model to use. Can be \"tiny\", \"base\", \"small\", \"medium\", or \"large\".", "type": "string", "choices": ["tiny", "base", "small", "medium", "large"], "default": "tiny", "multivalued": false}, {"name": "modelLang", "description": "Language of the model to use, accepts two- or three-letter ISO 639 language codes, however Whisper only supports a subset of languages. If the language is not supported, error will be raised.For the full list of supported languages, see https://github.com/openai/whisper/blob/20231117/whisper/tokenizer.py . In addition to the langauge code, two-letter region codes can be added to the language code, e.g. \"en-US\" for US English. Note that the region code is only for compatibility and recording purpose, and Whisper neither detects regional dialects, nor use the given one for transcription. When the langauge code is not given, Whisper will run in langauge detection mode, and will use first few seconds of the audio to detect the language.", "type": "string", "default": "", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Whisper Wrapper", "description": "A CLAMS wrapper for Whisper-based ASR software originally developed by OpenAI.", "app_version": "v6", "mmif_version": "1.1.0", "analyzer_version": "20231117", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/whisper-wrapper/v6", "url": "https://github.com/clamsproject/app-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v2", "properties": {"timeUnit": "millisecond"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}, {"@type": "http://vocab.lappsgrid.org/Sentence"}], "parameters": [{"name": "modelSize", "description": "The size of the model to use. When `modelLand=en` is given, for non-`large` models, English-only models will be used instead of multilingual models for speed and accuracy. (For `large` models, English-only models are not available.)", "type": "string", "choices": ["tiny", true, "base", "b", "small", "s", "medium", "m", "large", "l", "large-v2", "l2", "large-v3", "l3"], "default": "tiny", "multivalued": false}, {"name": "modelLang", "description": "Language of the model to use, accepts two- or three-letter ISO 639 language codes, however Whisper only supports a subset of languages. If the language is not supported, error will be raised.For the full list of supported languages, see https://github.com/openai/whisper/blob/20231117/whisper/tokenizer.py . In addition to the langauge code, two-letter region codes can be added to the language code, e.g. \"en-US\" for US English. Note that the region code is only for compatibility and recording purpose, and Whisper neither detects regional dialects, nor use the given one for transcription. When the langauge code is not given, Whisper will run in langauge detection mode, and will use first few seconds of the audio to detect the language.", "type": "string", "default": "", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v4.0", "mmif_version": "1.1.0", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v4.0", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v2", "properties": {"timeUnit": "milliseconds", "labelset": ["bars", "slate", "chyron", "credits", "NEG"]}}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v1", "properties": {"timeUnit": "milliseconds", "labelset": ["bars", "slate", "chyron", "credits", "NEG"]}}], "parameters": [{"name": "startAt", "description": "Number of milliseconds into the video to start processing", "type": "integer", "default": 0, "multivalued": false}, {"name": "stopAt", "description": "Number of milliseconds into the video to stop processing", "type": "integer", "default": 10000000, "multivalued": false}, {"name": "sampleRate", "description": "Milliseconds between sampled frames", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minFrameScore", "description": "Minimum score for a still frame to be included in a TimeFrame", "type": "number", "default": 0.01, "multivalued": false}, {"name": "minTimeframeScore", "description": "Minimum score for a TimeFrame", "type": "number", "default": 0.5, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of sampled frames required for a TimeFrame", "type": "integer", "default": 2, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v4.1", "mmif_version": "1.0.2", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v4.1", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v3", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v2", "properties": {"timeUnit": "milliseconds", "labelset": ["B", "S", "S:H", "S:C", "S:D", "S:B", "S:G", "W", "L", "O", "M", "I", "N", "E", "P", "Y", "K", "G", "T", "F", "C", "R"]}}], "parameters": [{"name": "startAt", "description": "Number of milliseconds into the video to start processing", "type": "integer", "default": 0, "multivalued": false}, {"name": "stopAt", "description": "Number of milliseconds into the video to stop processing", "type": "integer", "default": 10000000, "multivalued": false}, {"name": "sampleRate", "description": "Milliseconds between sampled frames", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minFrameScore", "description": "Minimum score for a still frame to be included in a TimeFrame", "type": "number", "default": 0.01, "multivalued": false}, {"name": "minTimeframeScore", "description": "Minimum score for a TimeFrame", "type": "number", "default": 0.5, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of sampled frames required for a TimeFrame", "type": "integer", "default": 2, "multivalued": false}, {"name": "modelName", "description": "model name to use for classification", "type": "string", "choices": ["20240126-180026.convnext_lg.kfold_000", "20240212-132306.convnext_lg.kfold_000", "20240212-131937.convnext_tiny.kfold_000"], "default": "20240126-180026.convnext_lg.kfold_000", "multivalued": false}, {"name": "useStitcher", "description": "Use the stitcher after classifying the TimePoints", "type": "boolean", "default": true, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v4.2", "mmif_version": "1.0.2", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v4.2", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v3", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v2", "properties": {"timeUnit": "milliseconds", "labelset": ["B", "S", "S:H", "S:C", "S:D", "S:B", "S:G", "W", "L", "O", "M", "I", "N", "E", "P", "Y", "K", "G", "T", "F", "C", "R"]}}], "parameters": [{"name": "startAt", "description": "Number of milliseconds into the video to start processing", "type": "integer", "default": 0, "multivalued": false}, {"name": "stopAt", "description": "Number of milliseconds into the video to stop processing", "type": "integer", "default": 10000000, "multivalued": false}, {"name": "sampleRate", "description": "Milliseconds between sampled frames", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minFrameScore", "description": "Minimum score for a still frame to be included in a TimeFrame", "type": "number", "default": 0.01, "multivalued": false}, {"name": "minTimeframeScore", "description": "Minimum score for a TimeFrame", "type": "number", "default": 0.5, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of sampled frames required for a TimeFrame", "type": "integer", "default": 2, "multivalued": false}, {"name": "modelName", "description": "model name to use for classification", "type": "string", "choices": ["20240126-180026.convnext_lg.kfold_000", "20240212-132306.convnext_lg.kfold_000", "20240212-131937.convnext_tiny.kfold_000"], "default": "20240126-180026.convnext_lg.kfold_000", "multivalued": false}, {"name": "useStitcher", "description": "Use the stitcher after classifying the TimePoints", "type": "boolean", "default": true, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Pyscenedetect Wrapper", "description": "CLAMS app wraps PySceneDetect and performs shot boundary detection on input videos", "app_version": "v3", "mmif_version": "1.0.2", "analyzer_version": "0.6.1", "app_license": "Apache2", "analyzer_license": "BSD-3", "identifier": "http://apps.clams.ai/pyscenedetect-wrapper/v3", "url": "https://github.com/clamsproject/app-pyscenedetect-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v3", "properties": {"label": "shot", "timeUnit": "frame"}}], "parameters": [{"name": "mode", "description": "pick a scene detector algorithm, see http://scenedetect.com/projects/Manual/en/latest/cli/detectors.html", "type": "string", "choices": ["content", "threshold", "adaptive"], "default": "content", "multivalued": false}, {"name": "threshold", "description": "threshold value to use in the detection algorithm. Note that the meaning of this numerical value differs for different detector algorithms.", "type": "number", "default": 27.0, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": 0, "multivalued": false}]}, {"name": "Simple Timepoints Stitcher", "description": "Stitches a sequence of `TimePoint` annotations into a sequence of `TimeFrame` annotations, performing simple smoothing of short peaks of positive labels.", "app_version": "v1.0", "mmif_version": "1.0.4", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/simple-timepoints-stitcher/v1.0", "url": "https://github.com/clamsproject/app-simple-timepoints-stitcher", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "description": "TimePoint annotations to be stitched. Must be \"exhaustive\" in that it should cover an entire single time period in the input document, with a uniform sample rate.", "properties": {"timePoint": "*", "classification": "*"}, "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "description": "Stitched TimeFrame annotations. Each TimeFrame annotation represents a continuous segment of timepoints and its `label` property is determined by the `labelMap` parameter (see `parameters` section). The `representatives` is a singleton list of the TimePoint annotation that has the highest score in the TimeFrame.", "properties": {"timeUnit": "milliseconds", "label": "*", "representatives": "*"}}], "parameters": [{"name": "labelMap", "description": "mapping of labels in the input annotations to new labels. Must be formatted as \"IN_LABEL:OUT_LABEL\" (with a colon). To pass multiple mappings, use this parameter multiple times. By default, all the input labels are passed as is, including any \"negative\" labels (with default value being no remapping at all). However, when at least one label is remapped, all the other \"unset\" labels are discarded asa negative label(\"-\").", "type": "map", "default": [], "multivalued": true}, {"name": "minTFDuration", "description": "minimum duration of a TimeFrame in milliseconds", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minTPScore", "description": "minimum score of a TimePoint to be considered as positive", "type": "number", "default": 0.1, "multivalued": false}, {"name": "minTFScore", "description": "minimum average score of TimePoints in a TimeFrame to be considered as positive", "type": "number", "default": 0.5, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Simple Timepoints Stitcher", "description": "Stitches a sequence of `TimePoint` annotations into a sequence of `TimeFrame` annotations, performing simple smoothing of short peaks of positive labels.", "app_version": "v1.1", "mmif_version": "1.0.4", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/simple-timepoints-stitcher/v1.1", "url": "https://github.com/clamsproject/app-simple-timepoints-stitcher", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "description": "TimePoint annotations to be stitched. Must be \"exhaustive\" in that it should cover an entire single time period in the input document, with a uniform sample rate.", "properties": {"timePoint": "*", "classification": "*"}, "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "description": "Stitched TimeFrame annotations. Each TimeFrame annotation represents a continuous segment of timepoints and its `label` property is determined by the `labelMap` parameter (see `parameters` section). The `representatives` is a singleton list of the TimePoint annotation that has the highest score in the TimeFrame.", "properties": {"timeUnit": "milliseconds", "label": "*", "representatives": "*"}}], "parameters": [{"name": "labelMap", "description": "mapping of labels in the input annotations to new labels. Must be formatted as \"IN_LABEL:OUT_LABEL\" (with a colon). To pass multiple mappings, use this parameter multiple times. By default, all the input labels are passed as is, including any \"negative\" labels (with default value being no remapping at all). However, when at least one label is remapped, all the other \"unset\" labels are discarded asa negative label(\"-\").", "type": "map", "default": [], "multivalued": true}, {"name": "minTFDuration", "description": "minimum duration of a TimeFrame in milliseconds", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minTPScore", "description": "minimum score of a TimePoint to be considered as positive", "type": "number", "default": 0.1, "multivalued": false}, {"name": "minTFScore", "description": "minimum average score of TimePoints in a TimeFrame to be considered as positive", "type": "number", "default": 0.5, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v4.3", "mmif_version": "1.0.4", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v4.3", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "properties": {"timeUnit": "milliseconds", "labelset": ["B", "S", "S:H", "S:C", "S:D", "S:B", "S:G", "W", "L", "O", "M", "I", "N", "E", "P", "Y", "K", "G", "T", "F", "C", "R"]}}], "parameters": [{"name": "startAt", "description": "Number of milliseconds into the video to start processing", "type": "integer", "default": 0, "multivalued": false}, {"name": "stopAt", "description": "Number of milliseconds into the video to stop processing", "type": "integer", "default": 10000000, "multivalued": false}, {"name": "sampleRate", "description": "Milliseconds between sampled frames", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minFrameScore", "description": "Minimum score for a still frame to be included in a TimeFrame", "type": "number", "default": 0.01, "multivalued": false}, {"name": "minTimeframeScore", "description": "Minimum score for a TimeFrame", "type": "number", "default": 0.5, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of sampled frames required for a TimeFrame", "type": "integer", "default": 2, "multivalued": false}, {"name": "modelName", "description": "model name to use for classification", "type": "string", "choices": ["20240126-180026.convnext_lg.kfold_000", "20240212-131937.convnext_tiny.kfold_000", "20240212-132306.convnext_lg.kfold_000"], "default": "20240126-180026.convnext_lg.kfold_000", "multivalued": false}, {"name": "useStitcher", "description": "Use the stitcher after classifying the TimePoints", "type": "boolean", "default": true, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v4.4", "mmif_version": "1.0.4", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v4.4", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "properties": {"timeUnit": "milliseconds", "labelset": ["B", "S", "S:H", "S:C", "S:D", "S:B", "S:G", "W", "L", "O", "M", "I", "N", "E", "P", "Y", "K", "G", "T", "F", "C", "R"]}}], "parameters": [{"name": "startAt", "description": "Number of milliseconds into the video to start processing", "type": "integer", "default": 0, "multivalued": false}, {"name": "stopAt", "description": "Number of milliseconds into the video to stop processing", "type": "integer", "default": 10000000, "multivalued": false}, {"name": "sampleRate", "description": "Milliseconds between sampled frames", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minFrameScore", "description": "Minimum score for a still frame to be included in a TimeFrame", "type": "number", "default": 0.01, "multivalued": false}, {"name": "minTimeframeScore", "description": "Minimum score for a TimeFrame", "type": "number", "default": 0.5, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of sampled frames required for a TimeFrame", "type": "integer", "default": 2, "multivalued": false}, {"name": "modelName", "description": "model name to use for classification", "type": "string", "choices": ["20240126-180026.convnext_lg.kfold_000", "20240409-091401.convnext_lg.kfold_013", "20240409-093229.convnext_tiny.kfold_012", "20240212-131937.convnext_tiny.kfold_000", "20240212-132306.convnext_lg.kfold_000"], "default": "20240409-091401.convnext_lg.kfold_013", "multivalued": false}, {"name": "useStitcher", "description": "Use the stitcher after classifying the TimePoints", "type": "boolean", "default": true, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Simple Timepoints Stitcher", "description": "Stitches a sequence of `TimePoint` annotations into a sequence of `TimeFrame` annotations, performing simple smoothing of short peaks of positive labels.", "app_version": "v1.2", "mmif_version": "1.0.4", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/simple-timepoints-stitcher/v1.2", "url": "https://github.com/clamsproject/app-simple-timepoints-stitcher", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "description": "TimePoint annotations to be stitched. Must be \"exhaustive\" in that it should cover an entire single time period in the input document, with a uniform sample rate.", "properties": {"timePoint": "*", "classification": "*"}, "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "description": "Stitched TimeFrame annotations. Each TimeFrame annotation represents a continuous segment of timepoints and its `label` property is determined by the `labelMap` parameter (see `parameters` section). The `representatives` is a singleton list of the TimePoint annotation that has the highest score in the TimeFrame.", "properties": {"timeUnit": "milliseconds", "label": "*", "representatives": "*"}}], "parameters": [{"name": "labelMap", "description": "mapping of labels in the input annotations to new labels. Must be formatted as \"IN_LABEL:OUT_LABEL\" (with a colon). To pass multiple mappings, use this parameter multiple times. By default, all the input labels are passed as is, including any \"negative\" labels (with default value being no remapping at all). However, when at least one label is remapped, all the other \"unset\" labels are discarded asa negative label(\"-\").", "type": "map", "default": [], "multivalued": true}, {"name": "minTFDuration", "description": "minimum duration of a TimeFrame in milliseconds", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minTPScore", "description": "minimum score of a TimePoint to be considered as positive", "type": "number", "default": 0.1, "multivalued": false}, {"name": "minTFScore", "description": "minimum average score of TimePoints in a TimeFrame to be considered as positive", "type": "number", "default": 0.5, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Simple Timepoints Stitcher", "description": "Stitches a sequence of `TimePoint` annotations into a sequence of `TimeFrame` annotations, performing simple smoothing of short peaks of positive labels.", "app_version": "v1.3", "mmif_version": "1.0.4", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/simple-timepoints-stitcher/v1.3", "url": "https://github.com/clamsproject/app-simple-timepoints-stitcher", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "description": "TimePoint annotations to be stitched. Must be \"exhaustive\" in that it should cover an entire single time period in the input document, with a uniform sample rate.", "properties": {"timePoint": "*", "classification": "*"}, "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "description": "Stitched TimeFrame annotations. Each TimeFrame annotation represents a continuous segment of timepoints and its `label` property is determined by the `labelMap` parameter (see `parameters` section). The `representatives` is a singleton list of the TimePoint annotation that has the highest score in the TimeFrame.", "properties": {"timeUnit": "milliseconds", "label": "*", "representatives": "*"}}], "parameters": [{"name": "labelMap", "description": "mapping of labels in the input annotations to new labels. Must be formatted as \"IN_LABEL:OUT_LABEL\" (with a colon). To pass multiple mappings, use this parameter multiple times. By default, all the input labels are passed as is, including any \"negative\" labels (with default value being no remapping at all). However, when at least one label is remapped, all the other \"unset\" labels are discarded asa negative label(\"-\").", "type": "map", "default": [], "multivalued": true}, {"name": "minTFDuration", "description": "minimum duration of a TimeFrame in milliseconds", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minTPScore", "description": "minimum score of a TimePoint to be considered as positive", "type": "number", "default": 0.1, "multivalued": false}, {"name": "minTFScore", "description": "minimum average score of TimePoints in a TimeFrame to be considered as positive", "type": "number", "default": 0.5, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "CLAMS docTR Wrapper", "description": "CLAMS app wraps the docTR, End-to-End OCR model, available at https://pypi.org/project/python-doctr . The model is capable of detecting text regions in the input image and recognizing text in the regions. The text-localized regions are organized hierarchically by the model into \"pages\" > \"blocks\" > \"lines\" > \"words\", and this CLAMS app translates them into `TextDocument`, `Paragraphs`, `Sentence`, and `Token` annotations to represent recognized text contents, then aligns them to `BoundingBox` annotations that represent the detected geometries. This hierarchical structure is also represented in the `TextDocument` annotation output as two newlines (`\\n\\n`) between \"paragraphs\", one newline (`\\n`) between the \"lines\", and one space (\" \") between the \"words\". For the text recognition, the model is internally configured to use the \"parseq\" recognition model, and only works with English text at the moment.", "app_version": "v1.0", "mmif_version": "1.0.4", "analyzer_version": "0.8.1", "app_license": "Apache 2.0", "analyzer_license": "Apache 2.0", "identifier": "http://apps.clams.ai/doctr-wrapper/v1.0", "url": "https://github.com/clamsproject/app-doctr-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "description": "The Time frame annotation that represents the video segment to be processed. When `representatives` property is present, the app will process videos still frames at the underlying time point annotations that are referred to by the `representatives` property. Otherwise, the app will process the middle frame of the video segment.", "properties": {"representatives": "?"}, "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "description": "Fully serialized text content of the recognized text in the input images. Serialization isdone by concatenating `text` values of `Paragraph` annotations with two newline characters.", "properties": {"@lang": "en"}}, {"@type": "http://vocab.lappsgrid.org/Token", "description": "Translation of the recognized docTR \"words\" in the input images. `text` and `word` properties store the string values of the recognized text. The duplication is for keepingbackward compatibility and consistency with `Paragraph` and `Sentence` annotations.", "properties": {"text": "*", "word": "*"}}, {"@type": "http://vocab.lappsgrid.org/Sentence", "description": "Translation of the recognized docTR \"lines\" in the input images. `text` property stores the string value of space-joined words.", "properties": {"text": "*"}}, {"@type": "http://vocab.lappsgrid.org/Paragraph", "description": "Translation of the recognized docTR \"blocks\" in the input images. `text` property stores the string value of newline-joined sentences.", "properties": {"text": "*"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1", "description": "Alignments between 1) `TimePoint` <-> `TextDocument`, 2) `TimePoint` <-> `Token`/`Sentence`/`Paragraph`, 3) `BoundingBox` <-> `Token`/`Sentence`/`Paragraph`"}, {"@type": "http://mmif.clams.ai/vocabulary/BoundingBox/v4", "description": "Bounding boxes of the detected text regions in the input images. No corresponding box for the entire image (`TextDocument`) region", "properties": {"label": "text"}}], "parameters": [{"name": "tfLabel", "description": "The label of the TimeFrame annotation to be processed. By default (`[]`), all TimeFrame annotations will be processed, regardless of their `label` property values.", "type": "string", "default": [], "multivalued": true}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "CLAMS docTR Wrapper", "description": "CLAMS app wraps the [docTR, End-to-End OCR model](https://pypi.org/project/python-doctr). The model can detect text regions in the input image and recognize text in the regions (via parseq OCR model, only English is support at the moment). The text-localized regions are organized hierarchically by the model into \"pages\" > \"blocks\" > \"lines\" > \"words\", and this CLAMS app translates them into `TextDocument`, `Paragraphs`, `Sentence`, and `Token` annotations to represent recognized text contents. See descriptions for I/O types below  for details on how annotations are aligned to each other.", "app_version": "v1.1", "mmif_version": "1.0.4", "analyzer_version": "0.8.1", "app_license": "Apache 2.0", "analyzer_license": "Apache 2.0", "identifier": "http://apps.clams.ai/doctr-wrapper/v1.1", "url": "https://github.com/clamsproject/app-doctr-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "description": "The Time frame annotation that represents the video segment to be processed. When `representatives` property is present, the app will process videos still frames at the underlying time point annotations that are referred to by the `representatives` property. Otherwise, the app will process the middle frame of the video segment.", "properties": {"representatives": "?"}, "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "description": "Fully serialized text content of the recognized text in the input images. Serialization isdone by concatenating `text` values of `Paragraph` annotations with two newline characters.", "properties": {"@lang": "en"}}, {"@type": "http://vocab.lappsgrid.org/Token", "description": "Translation of the recognized docTR \"words\" in the input images. `text` and `word` properties store the string values of the recognized text. The duplication is for keepingbackward compatibility and consistency with `Paragraph` and `Sentence` annotations.", "properties": {"text": "*", "word": "*"}}, {"@type": "http://vocab.lappsgrid.org/Sentence", "description": "Translation of the recognized docTR \"lines\" in the input images. `text` property stores the string value of space-joined words.", "properties": {"text": "*"}}, {"@type": "http://vocab.lappsgrid.org/Paragraph", "description": "Translation of the recognized docTR \"blocks\" in the input images. `text` property stores the string value of newline-joined sentences.", "properties": {"text": "*"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1", "description": "Alignments between 1) `TimePoint` <-> `TextDocument`, 2) `TimePoint` <-> `Token`/`Sentence`/`Paragraph`, 3) `BoundingBox` <-> `Token`/`Sentence`/`Paragraph`"}, {"@type": "http://mmif.clams.ai/vocabulary/BoundingBox/v4", "description": "Bounding boxes of the detected text regions in the input images. No corresponding box for the entire image (`TextDocument`) region", "properties": {"label": "text"}}], "parameters": [{"name": "tfLabel", "description": "The label of the TimeFrame annotation to be processed. By default (`[]`), all TimeFrame annotations will be processed, regardless of their `label` property values.", "type": "string", "default": [], "multivalued": true}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Simple Timepoints Stitcher", "description": "Stitches a sequence of `TimePoint` annotations into a sequence of `TimeFrame` annotations, performing simple smoothing of short peaks of positive labels.", "app_version": "v2.0", "mmif_version": "1.0.4", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/simple-timepoints-stitcher/v2.0", "url": "https://github.com/clamsproject/app-simple-timepoints-stitcher", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "description": "TimePoint annotations to be stitched. Must be \"exhaustive\" in that it should cover an entire single time period in the input document, with a uniform sample rate.", "properties": {"timePoint": "*", "classification": "*"}, "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "description": "Stitched TimeFrame annotations. Each TimeFrame annotation represents a continuous segment of timepoints and its `label` property is determined by the `labelMap` parameter (see `parameters` section). The `representatives` is a singleton list of the TimePoint annotation that has the highest score in the TimeFrame.", "properties": {"timeUnit": "milliseconds", "label": "*", "representatives": "*"}}], "parameters": [{"name": "labelMap", "description": "mapping of labels in the input annotations to new labels. Must be formatted as \"IN_LABEL:OUT_LABEL\" (with a colon). To pass multiple mappings, use this parameter multiple times. By default, all the input labels are passed as is, including any \"negative\" labels (with default value being no remapping at all). However, when at least one label is remapped, all the other \"unset\" labels are discarded as the negative label(\"-\").", "type": "map", "default": [], "multivalued": true}, {"name": "minTFDuration", "description": "minimum duration of a TimeFrame in milliseconds", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minTPScore", "description": "minimum score of a TimePoint to be considered as positive", "type": "number", "default": 0.1, "multivalued": false}, {"name": "minTFScore", "description": "minimum average score of TimePoints in a TimeFrame to be considered as positive", "type": "number", "default": 0.5, "multivalued": false}, {"name": "labelMapPreset", "description": "preset of label mappings. If not `null`, this parameter will override the `labelMap` parameter. Available presets are:\n- `null`: `None`\n- `swt-v4-4way`: `['B:bars', 'S:slate', 'S-H:slate', 'S-C:slate', 'S-D:slate', 'S-G:slate', 'I:chyron', 'N:chyron', 'Y:chyron', 'C:credit', 'R:credit']`\n- `swt-v4-6way`: `['B:bars', 'S:slate', 'S-H:slate', 'S-C:slate', 'S-D:slate', 'S-G:slate', 'I:chyron', 'N:chyron', 'Y:chyron', 'C:credit', 'R:credit', 'E:other_text', 'K:other_text', 'G:other_text', 'T:other_text', 'F:other_text', 'W:other_opening', 'L:other_opening', 'O:other_opening', 'M:other_opening']`", "type": "string", "choices": ["null", "swt-v4-4way", "swt-v4-6way"], "default": "null", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "inaSpeechSegmenter Wrapper", "description": "inaSpeechSegmenter is a CNN-based audio segmentation toolkit. The original software can be found at https://github.com/ina-foss/inaSpeechSegmenter .", "app_version": "v1.2", "mmif_version": "1.0.4", "analyzer_version": "0.7.6", "app_license": "MIT", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/inaspeechsegmenter-wrapper/v1.2", "url": "https://github.com/clamsproject/app-inaspeechsegmenter-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeunit": "milliseconds"}}], "parameters": [{"name": "minDuration", "description": "minimum duration of a TimeFrame in milliseconds", "type": "integer", "default": 0, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "inaSpeechSegmenter Wrapper", "description": "inaSpeechSegmenter is a CNN-based audio segmentation toolkit. The original software can be found at https://github.com/ina-foss/inaSpeechSegmenter .", "app_version": "v2.0", "mmif_version": "1.0.4", "analyzer_version": "0.7.6", "app_license": "MIT", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/inaspeechsegmenter-wrapper/v2.0", "url": "https://github.com/clamsproject/app-inaspeechsegmenter-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "description": "The INA semgmenter uses 5-way classification (['noEnergy', 'female', 'male', 'noise', 'music']) and this wrapper remaps the labels to ['silence', 'speech', 'noise', 'music'], by 1) renaming `noEnergy` to `silence` 2) collapsing `female` and `male` into `speech` (leaving additional `gender` property). Note that the time frame annotations do not exhaustively cover the input audio, but only the segments.", "properties": {"timeunit": "milliseconds", "labelset": ["silence", "speech", "noise", "music"]}}], "parameters": [{"name": "minTFDuration", "description": "minimum duration of a TimeFrame in milliseconds", "type": "integer", "default": 0, "multivalued": false}, {"name": "silenceRatio", "description": "percentage ratio (0-100) of audio energy to to determine silence, ratio to mean every of the input audio.", "type": "integer", "default": 3, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v5.0", "mmif_version": "1.0.4", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v5.0", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "properties": {"timeUnit": "milliseconds", "labelset": ["B", "S", "S:H", "S:C", "S:D", "S:B", "S:G", "W", "L", "O", "M", "I", "N", "E", "P", "Y", "K", "G", "T", "F", "C", "R"]}}], "parameters": [{"name": "startAt", "description": "Number of milliseconds into the video to start processing", "type": "integer", "default": 0, "multivalued": false}, {"name": "stopAt", "description": "Number of milliseconds into the video to stop processing", "type": "integer", "default": 9223372036854775807, "multivalued": false}, {"name": "sampleRate", "description": "Milliseconds between sampled frames", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minFrameScore", "description": "Minimum score for a still frame to be included in a TimeFrame", "type": "number", "default": 0.01, "multivalued": false}, {"name": "minTimeframeScore", "description": "Minimum score for a TimeFrame", "type": "number", "default": 0.5, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of sampled frames required for a TimeFrame", "type": "integer", "default": 2, "multivalued": false}, {"name": "modelName", "description": "model name to use for classification", "type": "string", "choices": ["20240212-132306.convnext_lg", "20240409-093229.convnext_tiny", "20240409-091401.convnext_lg", "20240126-180026.convnext_lg", "20240212-131937.convnext_tiny"], "default": "20240409-091401.convnext_lg", "multivalued": false}, {"name": "useStitcher", "description": "Use the stitcher after classifying the TimePoints", "type": "boolean", "default": true, "multivalued": false}, {"name": "allowOverlap", "description": "Allow overlapping time frames", "type": "boolean", "default": true, "multivalued": false}, {"name": "map", "description": "Mapping of a label in the input annotations to a new label. Must be formatted as IN_LABEL:OUT_LABEL (with a colon). To pass multiple mappings, use this parameter multiple times. By default, all the input labels are passed as is, including any negative labels (with default value being no remapping at all). However, when at least one label is remapped, all the other \"unset\" labels are discarded as a negative label.", "type": "map", "default": ["B:bars", "S:slate", "S-H:slate", "S-C:slate", "S-D:slate", "S-G:slate", "W:other_opening", "L:other_opening", "O:other_opening", "M:other_opening", "I:chyron", "N:chyron", "Y:chyron", "C:credit", "R:credit", "E:other_text", "K:other_text", "G:other_text", "T:other_text", "F:other_text"], "multivalued": true}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Llava Captioner", "description": "Applies llava to video frames.", "app_version": "v1.0", "mmif_version": "1.0.4", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/llava-captioner/v1.0", "url": "https://github.com/clamsproject/app-llava-captioner", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}], "parameters": [{"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Llava Captioner", "description": "Applies llava to video frames.", "app_version": "v1.1", "mmif_version": "1.0.4", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/llava-captioner/v1.1", "url": "https://github.com/clamsproject/app-llava-captioner", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}], "parameters": [{"name": "defaultPrompt", "description": "default prompt to use for timeframes not specified in the promptMap. If set to `-`, timeframes not specified in the promptMap will be skipped.", "type": "string", "default": "What is shown in this video frame?", "multivalued": false}, {"name": "promptMap", "description": "mapping of labels of the input timeframe annotations to new prompts. Must be formatted as \"IN_LABEL:PROMPT\" (with a colon). To pass multiple mappings, use this parameter multiple times. By default, any timeframe labels not mapped to a prompt will be used with the defaultprompt. In order to skip timeframes with a particular label, pass `-` as the prompt value.in order to skip all timeframes not specified in the promptMap, set the defaultPromptparameter to `-`", "type": "map", "default": [], "multivalued": true}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "CLAMS wrapper for spaCy NLP", "description": "Apply spaCy NLP to all text documents in a MMIF file.", "app_version": "v1.2", "mmif_version": "1.0.4", "analyzer_version": "3.6", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/spacy-wrapper/v1.2", "url": "https://github.com/clamsproject/app-spacy-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "required": true}, {"@type": "http://vocab.lappsgrid.org/Token", "required": false}], "output": [{"@type": "http://vocab.lappsgrid.org/Token"}, {"@type": "http://vocab.lappsgrid.org/Token#pos"}, {"@type": "http://vocab.lappsgrid.org/Token#lemma"}, {"@type": "http://vocab.lappsgrid.org/NounChunk"}, {"@type": "http://vocab.lappsgrid.org/Sentence"}, {"@type": "http://vocab.lappsgrid.org/NamedEntity"}], "parameters": [{"name": "pretokenized", "description": "Boolean parameter to set the app to use existing tokenization, if available, for text documents for NLP processing. Useful to process ASR documents, for example.", "type": "boolean", "default": false, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "EAST Text Detection", "description": "OpenCV-based text localization app that used EAST text detection model. Please visit the source code repository for full documentation.", "app_version": "v1.2", "mmif_version": "1.0.4", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/east-textdetection/v1.2", "url": "https://github.com/clamsproject/app-east-textdetection", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/ImageDocument/v1", "required": true}], {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "required": false}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/BoundingBox/v4", "properties": {"bboxtype": "text"}}], "parameters": [{"name": "timeUnit", "description": "Unit for time points in the output. Only works with VideoDocument input.", "type": "string", "choices": ["frames", "seconds", "milliseconds"], "default": "frames", "multivalued": false}, {"name": "frameType", "description": "Segments of video to run on. Only works with VideoDocument input and TimeFrame input. Empty value means run on the every frame types.", "type": "string", "choices": ["", "slate", "chyron", "rolling-credit"], "default": [""], "multivalued": true}, {"name": "sampleRate", "description": "Frequency to sample frames. Only works with VideoDocument input, and without TimeFrame input. (when `TimeFrame` annotation is found, this parameter is ignored.)", "type": "integer", "default": 30, "multivalued": false}, {"name": "stopAt", "description": "Frame number to stop running. Only works with VideoDocument input. The default is roughly 2 hours of video at 30fps.", "type": "integer", "default": 216000, "multivalued": false}, {"name": "mergeBoxes", "description": "if True, creates a single merged bounding box from all detected boxes.", "type": "boolean", "default": false, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Whisper Wrapper", "description": "A CLAMS wrapper for Whisper-based ASR software originally developed by OpenAI.", "app_version": "v7", "mmif_version": "1.0.4", "analyzer_version": "20231117", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/whisper-wrapper/v7", "url": "https://github.com/clamsproject/app-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}, {"@type": "http://vocab.lappsgrid.org/Sentence"}], "parameters": [{"name": "modelSize", "description": "The size of the model to use. When `modelLang=en` is given, for non-`large` models, English-only models will be used instead of multilingual models for speed and accuracy. (For `large` models, English-only models are not available.)", "type": "string", "choices": ["tiny", true, "base", "b", "small", "s", "medium", "m", "large", "l", "large-v2", "l2", "large-v3", "l3"], "default": "tiny", "multivalued": false}, {"name": "modelLang", "description": "Language of the model to use, accepts two- or three-letter ISO 639 language codes, however Whisper only supports a subset of languages. If the language is not supported, error will be raised.For the full list of supported languages, see https://github.com/openai/whisper/blob/20231117/whisper/tokenizer.py . In addition to the langauge code, two-letter region codes can be added to the language code, e.g. \"en-US\" for US English. Note that the region code is only for compatibility and recording purpose, and Whisper neither detects regional dialects, nor use the given one for transcription. When the langauge code is not given, Whisper will run in langauge detection mode, and will use first few seconds of the audio to detect the language.", "type": "string", "default": "", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Simple Timepoints Stitcher", "description": "Stitches a sequence of `TimePoint` annotations into a sequence of `TimeFrame` annotations, performing simple smoothing of short peaks of positive labels.", "app_version": "v2.1", "mmif_version": "1.0.5", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/simple-timepoints-stitcher/v2.1", "url": "https://github.com/clamsproject/app-simple-timepoints-stitcher", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "description": "TimePoint annotations to be stitched. Must be \"exhaustive\" in that it should cover an entire single time period in the input document, with a uniform sample rate.", "properties": {"timePoint": "*", "classification": "*"}, "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "description": "Stitched TimeFrame annotations. Each TimeFrame annotation represents a continuous segment of timepoints and its `label` property is determined by the `labelMap` parameter (see `parameters` section). The `representatives` is a singleton list of the TimePoint annotation that has the highest score in the TimeFrame.", "properties": {"timeUnit": "milliseconds", "label": "*", "representatives": "*"}}], "parameters": [{"name": "labelMap", "description": "mapping of labels in the input annotations to new labels. Must be formatted as \"IN_LABEL:OUT_LABEL\" (with a colon). To pass multiple mappings, use this parameter multiple times. By default, all the input labels are passed as is, including any \"negative\" labels (with default value being no remapping at all). However, when at least one label is remapped, all the other \"unset\" labels are discarded as the negative label(\"-\").", "type": "map", "default": [], "multivalued": true}, {"name": "minTFDuration", "description": "minimum duration of a TimeFrame in milliseconds", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minTPScore", "description": "minimum score of a TimePoint to be considered as positive", "type": "number", "default": 0.1, "multivalued": false}, {"name": "minTFScore", "description": "minimum average score of TimePoints in a TimeFrame to be considered as positive", "type": "number", "default": 0.5, "multivalued": false}, {"name": "labelMapPreset", "description": "preset of label mappings. If not `null`, this parameter will override the `labelMap` parameter. Available presets are:\n- `null`: `None`\n- `swt-v4-4way`: `['B:bars', 'S:slate', 'I:chyron', 'N:chyron', 'Y:chyron', 'C:credits', 'R:credits']`\n- `swt-v4-6way`: `['B:bars', 'S:slate', 'I:chyron', 'N:chyron', 'Y:chyron', 'C:credits', 'R:credits', 'E:other_text', 'K:other_text', 'G:other_text', 'T:other_text', 'F:other_text', 'W:other_opening', 'L:other_opening', 'O:other_opening', 'M:other_opening']`", "type": "string", "choices": ["null", "swt-v4-4way", "swt-v4-6way"], "default": "null", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v5.1", "mmif_version": "1.0.5", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v5.1", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "properties": {"timeUnit": "milliseconds", "labelset": ["B", "S", "W", "L", "O", "M", "I", "N", "E", "P", "Y", "K", "G", "T", "F", "C", "R"]}}], "parameters": [{"name": "startAt", "description": "Number of milliseconds into the video to start processing", "type": "integer", "default": 0, "multivalued": false}, {"name": "stopAt", "description": "Number of milliseconds into the video to stop processing", "type": "integer", "default": 9223372036854775807, "multivalued": false}, {"name": "sampleRate", "description": "Milliseconds between sampled frames", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minFrameScore", "description": "Minimum score for a still frame to be included in a TimeFrame", "type": "number", "default": 0.01, "multivalued": false}, {"name": "minTimeframeScore", "description": "Minimum score for a TimeFrame", "type": "number", "default": 0.5, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of sampled frames required for a TimeFrame", "type": "integer", "default": 2, "multivalued": false}, {"name": "modelName", "description": "model name to use for classification", "type": "string", "choices": ["20240626-205803.convnext_tiny", "20240626-205715.convnext_lg"], "default": "20240626-205715.convnext_lg", "multivalued": false}, {"name": "useStitcher", "description": "Use the stitcher after classifying the TimePoints", "type": "boolean", "default": true, "multivalued": false}, {"name": "allowOverlap", "description": "Allow overlapping time frames", "type": "boolean", "default": true, "multivalued": false}, {"name": "map", "description": "Mapping of a label in the input annotations to a new label. Must be formatted as IN_LABEL:OUT_LABEL (with a colon). To pass multiple mappings, use this parameter multiple times. By default, all the input labels are passed as is, including any negative labels (with default value being no remapping at all). However, when at least one label is remapped, all the other \"unset\" labels are discarded as a negative label.", "type": "map", "default": ["B:bars", "S:slate", "I:chyron", "N:chyron", "Y:chyron", "C:credits", "R:credits", "W:other_opening", "L:other_opening", "O:other_opening", "M:other_opening", "E:other_text", "K:other_text", "G:other_text", "T:other_text", "F:other_text"], "multivalued": true}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Text Slicer", "description": "Slice text snippets from a provided text document given time frames", "app_version": "v1.0", "mmif_version": "1.0.5", "app_license": "Apache2", "identifier": "http://apps.clams.ai/text-slicer/v1.0", "url": "https://github.com/clamsproject/app-text-slicer", "input": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}], "parameters": [{"name": "containLabel", "description": "A list of labels that user expect TimeFrames contain.\nLabels can be chosen from but not limited to:\n['bars', 'tones', 'bars-and-tones','speech','noise',\n 'music', 'slate', 'chyron', 'lower-third', 'credits']\nUsers are required to select at least one label. Otherwise, errors would be thrown instead", "type": "string", "multivalued": true}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "CLAMS wrapper for spaCy NLP", "description": "Apply spaCy NLP to all text documents in a MMIF file.", "app_version": "v2.0", "mmif_version": "1.0.5", "analyzer_version": "3.6", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/spacy-wrapper/v2.0", "url": "https://github.com/clamsproject/app-spacy-wrapper", "input": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "required": true}, {"@type": "http://vocab.lappsgrid.org/Token", "required": false}], "output": [{"@type": "http://vocab.lappsgrid.org/Token"}, {"@type": "http://vocab.lappsgrid.org/Token#pos"}, {"@type": "http://vocab.lappsgrid.org/Token#lemma"}, {"@type": "http://vocab.lappsgrid.org/NounChunk"}, {"@type": "http://vocab.lappsgrid.org/Sentence"}, {"@type": "http://vocab.lappsgrid.org/NamedEntity"}], "parameters": [{"name": "pretokenized", "description": "Boolean parameter to set the app to use existing tokenization, if available, for text documents for NLP processing. Useful to process ASR documents, for example.", "type": "boolean", "default": false, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Distil Whisper Wrapper", "description": "The wrapper of Distil-Whisper, avaliable models: distil-large-v3, distil-large-v2, distil-medium.en, distil-small.en. The default model is distil-small.en.", "app_version": "v1.0", "mmif_version": "1.0.4", "analyzer_version": "1.0", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/distil-whisper-wrapper/v1.0", "url": "https://github.com/clamsproject/app-distil-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "description": "Fully serialized text content of the recognized text in the input audio/video.", "properties": {"@lang": "en"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1", "description": "Alignments between 1) `TimeFrame` <-> `SENTENCE`, 2) `audio/video document` <-> `TextDocument`"}, {"@type": "http://vocab.lappsgrid.org/Sentence", "description": "The smallest recognized unit of distil-whisper. Normally a complete sentence."}], "parameters": [{"name": "modelSize", "description": "The size of the model to use. There are four size of model to use distil-large-v3, distil-large-v2, distil-medium.en, distil-small.en. You can also enter the abbreviation of the model as parameter. 'small' and 's' for distil-small.en; 'medium' and  'm' for distil-medium.en; 'large-v2' and 'l2' for distil-large-v2; 'large-v3' and 'l3' for distil-large-v3. The default model is distil-medium.en.)", "type": "string", "choices": ["distil-large-v3", "distil-large-v2", "distil-medium.en", "distil-small.en", "small", "s", "medium", "m", "large-v2", "l2", "large-v3", "l3"], "default": "distil-small.en", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Tfidf Keywordextractor", "description": "extract keywords of a text document according to TF-IDF values. IDF values and all features come from related pickle files in the current directory.App can either take a simple text document or take a MMIF file generated from the text slicer app.", "app_version": "v1.0", "mmif_version": "1.0.5", "analyzer_version": "", "app_license": "Apache 2.0", "analyzer_license": "", "identifier": "http://apps.clams.ai/tfidf-keywordextractor/v1.0", "url": "https://github.com/clamsproject/app-tfidf-keywordextractor", "input": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "description": "Default property 'text' stores the extracted keywords (string). Added property 'scores' stores keywords' TF-IDF values (float).", "properties": {"text": "keywords", "scores": "tfidf scores"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}], "parameters": [{"name": "topN", "description": "top n keywords to extract from the current textfile.", "type": "integer", "default": 10, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Distil Whisper Wrapper", "description": "The wrapper of Distil-Whisper, avaliable models: distil-large-v3, distil-large-v2, distil-medium.en, distil-small.en. The default model is distil-small.en.", "app_version": "v1.1", "mmif_version": "1.0.5", "analyzer_version": "1.0", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/distil-whisper-wrapper/v1.1", "url": "https://github.com/clamsproject/app-distil-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "description": "Fully serialized text content of the recognized text in the input audio/video.", "properties": {"@lang": "en"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1", "description": "Alignments between 1) `TimeFrame` <-> `SENTENCE`, 2) `audio/video document` <-> `TextDocument`"}, {"@type": "http://vocab.lappsgrid.org/Sentence", "description": "The smallest recognized unit of distil-whisper. Normally a complete sentence."}], "parameters": [{"name": "modelSize", "description": "The size of the model to use. There are four size of model to use distil-large-v3, distil-large-v2, distil-medium.en, distil-small.en. You can also enter the abbreviation of the model as parameter. 'small' and 's' for distil-small.en; 'medium' and  'm' for distil-medium.en; 'large-v2' and 'l2' for distil-large-v2; 'large-v3' and 'l3' for distil-large-v3. The default model is distil-medium.en.)", "type": "string", "choices": ["distil-large-v3", "distil-large-v2", "distil-medium.en", "distil-small.en", "small", "s", "medium", "m", "large-v2", "l2", "large-v3", "l3"], "default": "distil-small.en", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}, {"name": "runningTime", "description": "The running time of the app will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}, {"name": "hwFetch", "description": "The hardware information (architecture, GPU and vRAM) will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Whisper Wrapper", "description": "A CLAMS wrapper for Whisper-based ASR software originally developed by OpenAI.", "app_version": "v8", "mmif_version": "1.0.5", "analyzer_version": "20231117", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/whisper-wrapper/v8", "url": "https://github.com/clamsproject/app-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}, {"@type": "http://vocab.lappsgrid.org/Sentence"}], "parameters": [{"name": "modelSize", "description": "The size of the model to use. When `modelLang=en` is given, for non-`large` models, English-only models will be used instead of multilingual models for speed and accuracy. (For `large` models, English-only models are not available.) (also can be given as alias: tiny=t, base=b, small=s, medium=m, large=l, large-v2=l2, large-v3=l3)", "type": "string", "choices": ["tiny", true, "base", "b", "small", "s", "medium", "m", "large", "l", "large-v2", "l2", "large-v3", "l3"], "default": "tiny", "multivalued": false}, {"name": "modelLang", "description": "Language of the model to use, accepts two- or three-letter ISO 639 language codes, however Whisper only supports a subset of languages. If the language is not supported, error will be raised.For the full list of supported languages, see https://github.com/openai/whisper/blob/20231117/whisper/tokenizer.py . In addition to the langauge code, two-letter region codes can be added to the language code, e.g. \"en-US\" for US English. Note that the region code is only for compatibility and recording purpose, and Whisper neither detects regional dialects, nor use the given one for transcription. When the langauge code is not given, Whisper will run in langauge detection mode, and will use first few seconds of the audio to detect the language.", "type": "string", "default": "", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}, {"name": "runningTime", "description": "The running time of the app will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}, {"name": "hwFetch", "description": "The hardware information (architecture, GPU and vRAM) will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v6.0", "mmif_version": "1.0.5", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v6.0", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "properties": {"timeUnit": "milliseconds", "labelset": ["B", "S", "W", "L", "O", "M", "I", "N", "E", "P", "Y", "K", "G", "T", "F", "C", "R"]}}], "parameters": [{"name": "startAt", "description": "Number of milliseconds into the video to start processing", "type": "integer", "default": 0, "multivalued": false}, {"name": "stopAt", "description": "Number of milliseconds into the video to stop processing", "type": "integer", "default": 9223372036854775807, "multivalued": false}, {"name": "sampleRate", "description": "Milliseconds between sampled frames", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minFrameScore", "description": "Minimum score for a still frame to be included in a TimeFrame", "type": "number", "default": 0.01, "multivalued": false}, {"name": "minTimeframeScore", "description": "Minimum score for a TimeFrame", "type": "number", "default": 0.5, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of sampled frames required for a TimeFrame", "type": "integer", "default": 2, "multivalued": false}, {"name": "modelName", "description": "model name to use for classification", "type": "string", "choices": ["convnext_tiny", "convnext_lg"], "default": "convnext_lg", "multivalued": false}, {"name": "usePosModel", "description": "Use the model trained with positional features", "type": "boolean", "default": true, "multivalued": false}, {"name": "useStitcher", "description": "Use the stitcher after classifying the TimePoints", "type": "boolean", "default": true, "multivalued": false}, {"name": "allowOverlap", "description": "Allow overlapping time frames", "type": "boolean", "default": true, "multivalued": false}, {"name": "map", "description": "Mapping of a label in the input annotations to a new label. Must be formatted as IN_LABEL:OUT_LABEL (with a colon). To pass multiple mappings, use this parameter multiple times. By default, all the input labels are passed as is, including any negative labels (with default value being no remapping at all). However, when at least one label is remapped, all the other \"unset\" labels are discarded as a negative label.", "type": "map", "default": ["B:bars", "S:slate", "I:chyron", "N:chyron", "Y:chyron", "C:credits", "R:credits", "W:other_opening", "L:other_opening", "O:other_opening", "M:other_opening", "E:other_text", "K:other_text", "G:other_text", "T:other_text", "F:other_text"], "multivalued": true}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}, {"name": "runningTime", "description": "The running time of the app will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}, {"name": "hwFetch", "description": "The hardware information (architecture, GPU and vRAM) will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits.", "app_version": "v6.1", "mmif_version": "1.0.5", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v6.1", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "properties": {"timeUnit": "milliseconds", "labelset": ["B", "S", "W", "L", "O", "M", "I", "N", "E", "P", "Y", "K", "G", "T", "F", "C", "R"]}}], "parameters": [{"name": "startAt", "description": "Number of milliseconds into the video to start processing", "type": "integer", "default": 0, "multivalued": false}, {"name": "stopAt", "description": "Number of milliseconds into the video to stop processing", "type": "integer", "default": 9223372036854775807, "multivalued": false}, {"name": "sampleRate", "description": "Milliseconds between sampled frames", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minFrameScore", "description": "Minimum score for a still frame to be included in a TimeFrame", "type": "number", "default": 0.01, "multivalued": false}, {"name": "minTimeframeScore", "description": "Minimum score for a TimeFrame", "type": "number", "default": 0.5, "multivalued": false}, {"name": "minFrameCount", "description": "Minimum number of sampled frames required for a TimeFrame", "type": "integer", "default": 2, "multivalued": false}, {"name": "modelName", "description": "model name to use for classification", "type": "string", "choices": ["convnext_tiny", "convnext_lg"], "default": "convnext_lg", "multivalued": false}, {"name": "usePosModel", "description": "Use the model trained with positional features", "type": "boolean", "default": true, "multivalued": false}, {"name": "useStitcher", "description": "Use the stitcher after classifying the TimePoints", "type": "boolean", "default": true, "multivalued": false}, {"name": "allowOverlap", "description": "Allow overlapping time frames", "type": "boolean", "default": true, "multivalued": false}, {"name": "map", "description": "Mapping of a label in the input annotations to a new label. Must be formatted as IN_LABEL:OUT_LABEL (with a colon). To pass multiple mappings, use this parameter multiple times. By default, all the input labels are passed as is, including any negative labels (with default value being no remapping at all). However, when at least one label is remapped, all the other \"unset\" labels are discarded as a negative label.", "type": "map", "default": ["B:bars", "S:slate", "I:chyron", "N:chyron", "Y:chyron", "C:credits", "R:credits", "W:other_opening", "L:other_opening", "O:other_opening", "M:other_opening", "E:other_text", "K:other_text", "G:other_text", "T:other_text", "F:other_text"], "multivalued": true}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}, {"name": "runningTime", "description": "The running time of the app will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}, {"name": "hwFetch", "description": "The hardware information (architecture, GPU and vRAM) will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Simple Timepoints Stitcher", "description": "Stitches a sequence of `TimePoint` annotations into a sequence of `TimeFrame` annotations, performing simple smoothing of short peaks of positive labels.", "app_version": "v3.0", "mmif_version": "1.0.5", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/simple-timepoints-stitcher/v3.0", "url": "https://github.com/clamsproject/app-simple-timepoints-stitcher", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "description": "TimePoint annotations to be stitched. Must be \"exhaustive\" in that it should cover an entire single time period in the input document, with a uniform sample rate.", "properties": {"timePoint": "*", "classification": "*"}, "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "description": "Stitched TimeFrame annotations. Each TimeFrame annotation represents a continuous segment of timepoints and its `label` property is determined by the `labelMap` parameter (see `parameters` section). The `representatives` is a singleton list of the TimePoint annotation that has the highest score in the TimeFrame.", "properties": {"timeUnit": "milliseconds", "label": "*", "representatives": "*"}}], "parameters": [{"name": "labelMap", "description": "mapping of labels in the input annotations to new labels. Must be formatted as \"IN_LABEL:OUT_LABEL\" (with a colon). To pass multiple mappings, use this parameter multiple times. By default, all the input labels are passed as is, including any \"negative\" labels (with default value being no remapping at all). However, when at least one label is remapped, all the other \"unset\" labels are discarded as the negative label(\"-\").", "type": "map", "default": [], "multivalued": true}, {"name": "minTFDuration", "description": "minimum duration of a TimeFrame in milliseconds", "type": "integer", "default": 1000, "multivalued": false}, {"name": "minTPScore", "description": "minimum score of a TimePoint to be considered as positive", "type": "number", "default": 0.1, "multivalued": false}, {"name": "minTFScore", "description": "minimum average score of TimePoints in a TimeFrame to be considered as positive", "type": "number", "default": 0.5, "multivalued": false}, {"name": "labelMapPreset", "description": "preset of label mappings. If not `null`, this parameter will override the `labelMap` parameter. Available presets are:\n- `null`: `None`\n- `swt-v4-4way`: `['B:bars', 'S:slate', 'I:chyron', 'N:chyron', 'Y:chyron', 'C:credits', 'R:credits']`\n- `swt-v4-6way`: `['B:bars', 'S:slate', 'I:chyron', 'N:chyron', 'Y:chyron', 'C:credits', 'R:credits', 'E:other_text', 'K:other_text', 'G:other_text', 'T:other_text', 'F:other_text', 'W:other_opening', 'L:other_opening', 'O:other_opening', 'M:other_opening']`", "type": "string", "choices": ["null", "swt-v4-4way", "swt-v4-6way"], "default": "null", "multivalued": false}, {"name": "allowOverlap", "description": "Allow overlapping time frames", "type": "boolean", "default": true, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}, {"name": "runningTime", "description": "The running time of the app will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}, {"name": "hwFetch", "description": "The hardware information (architecture, GPU and vRAM) will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Distil Whisper Wrapper", "description": "The wrapper of Distil-Whisper, avaliable models: distil-large-v3, distil-large-v2, distil-medium.en, distil-small.en. The default model is distil-small.en.", "app_version": "v1.2", "mmif_version": "1.0.5", "analyzer_version": "1.0", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/distil-whisper-wrapper/v1.2", "url": "https://github.com/clamsproject/app-distil-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1", "description": "Fully serialized text content of the recognized text in the input audio/video.", "properties": {"@lang": "en"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1", "description": "Alignments between 1) `TimeFrame` <-> `SENTENCE`, 2) `audio/video document` <-> `TextDocument`"}, {"@type": "http://vocab.lappsgrid.org/Sentence", "description": "The smallest recognized unit of distil-whisper. Normally a complete sentence."}], "parameters": [{"name": "modelSize", "description": "The size of the model to use. There are four size of model to use distil-large-v3, distil-large-v2, distil-medium.en, distil-small.en. You can also enter the abbreviation of the model as parameter. 'small' and 's' for distil-small.en; 'medium' and  'm' for distil-medium.en; 'large-v2' and 'l2' for distil-large-v2; 'large-v3' and 'l3' for distil-large-v3. The default model is distil-medium.en.)", "type": "string", "choices": ["distil-large-v3", "distil-large-v2", "distil-medium.en", "distil-small.en", "small", "s", "medium", "m", "large-v2", "l2", "large-v3", "l3"], "default": "distil-small.en", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}, {"name": "runningTime", "description": "The running time of the app will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}, {"name": "hwFetch", "description": "The hardware information (architecture, GPU and vRAM) will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Whisper Wrapper", "description": "A CLAMS wrapper for Whisper-based ASR software originally developed by OpenAI.", "app_version": "v9", "mmif_version": "1.0.5", "analyzer_version": "20231117", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/whisper-wrapper/v9", "url": "https://github.com/clamsproject/app-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}, {"@type": "http://vocab.lappsgrid.org/Sentence"}], "parameters": [{"name": "modelSize", "description": "The size of the model to use. When `modelLang=en` is given, for non-`large` models, English-only models will be used instead of multilingual models for speed and accuracy. (For `large` models, English-only models are not available.) (also can be given as alias: tiny=t, base=b, small=s, medium=m, large=l, large-v2=l2, large-v3=l3)", "type": "string", "choices": ["tiny", true, "base", "b", "small", "s", "medium", "m", "large", "l", "large-v2", "l2", "large-v3", "l3"], "default": "tiny", "multivalued": false}, {"name": "modelLang", "description": "Language of the model to use, accepts two- or three-letter ISO 639 language codes, however Whisper only supports a subset of languages. If the language is not supported, error will be raised.For the full list of supported languages, see https://github.com/openai/whisper/blob/20231117/whisper/tokenizer.py . In addition to the langauge code, two-letter region codes can be added to the language code, e.g. \"en-US\" for US English. Note that the region code is only for compatibility and recording purpose, and Whisper neither detects regional dialects, nor use the given one for transcription. When the langauge code is not given, Whisper will run in langauge detection mode, and will use first few seconds of the audio to detect the language.", "type": "string", "default": "", "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}, {"name": "runningTime", "description": "The running time of the app will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}, {"name": "hwFetch", "description": "The hardware information (architecture, GPU and vRAM) will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Whisper Wrapper", "description": "A CLAMS wrapper for Whisper-based ASR software originally developed by OpenAI.", "app_version": "v10", "mmif_version": "1.0.5", "analyzer_version": "20231117", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/whisper-wrapper/v10", "url": "https://github.com/clamsproject/app-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}, {"@type": "http://vocab.lappsgrid.org/Sentence"}], "parameters": [{"name": "modelSize", "description": "The size of the model to use. When `modelLang=en` is given, for non-`large` models, English-only models will be used instead of multilingual models for speed and accuracy. (For `large` models, English-only models are not available.) (also can be given as alias: tiny=t, base=b, small=s, medium=m, large=l, large-v2=l2, large-v3=l3)", "type": "string", "choices": ["tiny", true, "base", "b", "small", "s", "medium", "m", "large", "l", "large-v2", "l2", "large-v3", "l3"], "default": "tiny", "multivalued": false}, {"name": "modelLang", "description": "Language of the model to use, accepts two- or three-letter ISO 639 language codes, however Whisper only supports a subset of languages. If the language is not supported, error will be raised.For the full list of supported languages, see https://github.com/openai/whisper/blob/20231117/whisper/tokenizer.py . In addition to the langauge code, two-letter region codes can be added to the language code, e.g. \"en-US\" for US English. Note that the region code is only for compatibility and recording purpose, and Whisper neither detects regional dialects, nor use the given one for transcription. When the langauge code is not given, Whisper will run in langauge detection mode, and will use first few seconds of the audio to detect the language.", "type": "string", "default": "", "multivalued": false}, {"name": "task", "description": "(from whisper CLI) whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')", "type": "string", "choices": ["transcribe", "translate"], "default": "transcribe", "multivalued": false}, {"name": "initialPrompt", "description": "(from whisper CLI) optional text to provide as a prompt for the first window.", "type": "string", "default": "", "multivalued": false}, {"name": "conditionOnPreviousText", "description": "(from whisper CLI) if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop", "type": "boolean", "default": true, "multivalued": false}, {"name": "noSpeechThreshold", "description": "(from whisper CLI) if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence", "type": "number", "default": 0.6, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}, {"name": "runningTime", "description": "The running time of the app will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}, {"name": "hwFetch", "description": "The hardware information (architecture, GPU and vRAM) will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Whisper Wrapper", "description": "A CLAMS wrapper for Whisper-based ASR software originally developed by OpenAI.", "app_version": "v11", "mmif_version": "1.0.5", "analyzer_version": "20240930", "app_license": "Apache 2.0", "analyzer_license": "MIT", "identifier": "http://apps.clams.ai/whisper-wrapper/v11", "url": "https://github.com/clamsproject/app-whisper-wrapper", "input": [[{"@type": "http://mmif.clams.ai/vocabulary/AudioDocument/v1", "required": true}, {"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}]], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TextDocument/v1"}, {"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/Alignment/v1"}, {"@type": "http://vocab.lappsgrid.org/Token"}, {"@type": "http://vocab.lappsgrid.org/Sentence"}], "parameters": [{"name": "modelSize", "description": "The size of the model to use. When `modelLang=en` is given, for non-`large` models, English-only models will be used instead of multilingual models for speed and accuracy. (For `large` models, English-only models are not available.) (also can be given as alias: tiny=t, base=b, small=s, medium=m, large=l, large-v2=l2, large-v3=l3, turbo=tu)", "type": "string", "choices": ["tiny", true, "base", "b", "small", "s", "medium", "m", "large", "l", "large-v2", "l2", "large-v3", "l3", "turbo", "tu"], "default": "tiny", "multivalued": false}, {"name": "modelLang", "description": "Language of the model to use, accepts two- or three-letter ISO 639 language codes, however Whisper only supports a subset of languages. If the language is not supported, error will be raised.For the full list of supported languages, see https://github.com/openai/whisper/blob/20240930/whisper/tokenizer.py . In addition to the langauge code, two-letter region codes can be added to the language code, e.g. \"en-US\" for US English. Note that the region code is only for compatibility and recording purpose, and Whisper neither detects regional dialects, nor use the given one for transcription. When the langauge code is not given, Whisper will run in langauge detection mode, and will use first few seconds of the audio to detect the language.", "type": "string", "default": "", "multivalued": false}, {"name": "task", "description": "(from whisper CLI) whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')", "type": "string", "choices": ["transcribe", "translate"], "default": "transcribe", "multivalued": false}, {"name": "initialPrompt", "description": "(from whisper CLI) optional text to provide as a prompt for the first window.", "type": "string", "default": "", "multivalued": false}, {"name": "conditionOnPreviousText", "description": "(from whisper CLI) if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop", "type": "boolean", "default": true, "multivalued": false}, {"name": "noSpeechThreshold", "description": "(from whisper CLI) if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence", "type": "number", "default": 0.6, "multivalued": false}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}, {"name": "runningTime", "description": "The running time of the app will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}, {"name": "hwFetch", "description": "The hardware information (architecture, GPU and vRAM) will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}]}, {"name": "Scenes-with-text Detection", "description": "Detects scenes with text, like slates, chyrons and credits. This app can run in three modes, depending on `useClassifier`, `useStitcher` parameters. When `useClassifier=True`, it runs in the \"TimePoint mode\" and generates TimePoint annotations. When `useStitcher=True`, it runs in the \"TimeFrame mode\" and generates TimeFrame annotations based on existing TimePoint annotations -- if no TimePoint is found, it produces an error. By default, it runs in the 'both' mode and first generates TimePoint annotations and then TimeFrame annotations on them.", "app_version": "v7.0", "mmif_version": "1.0.5", "app_license": "Apache 2.0", "identifier": "http://apps.clams.ai/swt-detection/v7.0", "url": "https://github.com/clamsproject/app-swt-detection", "input": [{"@type": "http://mmif.clams.ai/vocabulary/VideoDocument/v1", "required": true}], "output": [{"@type": "http://mmif.clams.ai/vocabulary/TimeFrame/v5", "properties": {"timeUnit": "milliseconds"}}, {"@type": "http://mmif.clams.ai/vocabulary/TimePoint/v4", "properties": {"timeUnit": "milliseconds", "labelset": ["B", "S", "W", "L", "O", "M", "I", "N", "E", "P", "Y", "K", "G", "T", "F", "C", "R"]}}], "parameters": [{"name": "useClassifier", "description": "Use the image classifier model to generate TimePoint annotations.", "type": "boolean", "default": true, "multivalued": false}, {"name": "tpModelName", "description": "Model name to use for classification, only applies when `useClassifier=true`.", "type": "string", "choices": ["convnext_tiny", "convnext_lg"], "default": "convnext_lg", "multivalued": false}, {"name": "tpUsePosModel", "description": "Use the model trained with positional features, only applies when `useClassifier=true`.", "type": "boolean", "default": true, "multivalued": false}, {"name": "tpStartAt", "description": "Number of milliseconds into the video to start processing, only applies when `useClassifier=true`.", "type": "integer", "default": 0, "multivalued": false}, {"name": "tpStopAt", "description": "Number of milliseconds into the video to stop processing, only applies when `useClassifier=true`.", "type": "integer", "default": 9223372036854775807, "multivalued": false}, {"name": "tpSampleRate", "description": "Milliseconds between sampled frames, only applies when `useClassifier=true`.", "type": "integer", "default": 1000, "multivalued": false}, {"name": "useStitcher", "description": "Use the stitcher after classifying the TimePoints.", "type": "boolean", "default": true, "multivalued": false}, {"name": "tfMinTPScore", "description": "Minimum score for a TimePoint to be included in a TimeFrame. A lower value will include more TimePoints in the TimeFrame (increasing recall in exchange for precision). Only applies when `useStitcher=true`.", "type": "number", "default": 0.5, "multivalued": false}, {"name": "tfMinTFScore", "description": "Minimum score for a TimeFrame. A lower value will include more TimeFrames in the output (increasing recall in exchange for precision). Only applies when `useStitcher=true`", "type": "number", "default": 0.9, "multivalued": false}, {"name": "tfMinTFDuration", "description": "Minimum duration of a TimeFrame in milliseconds, only applies when `useStitcher=true`.", "type": "integer", "default": 5000, "multivalued": false}, {"name": "tfAllowOverlap", "description": "Allow overlapping time frames, only applies when `useStitcher=true`", "type": "boolean", "default": false, "multivalued": false}, {"name": "tfDynamicSceneLabels", "description": "Labels that are considered dynamic scenes. For dynamic scenes, TimeFrame annotations contains multiple representative points to follow any changes in the scene. Only applies when `useStitcher=true`", "type": "string", "default": ["credit", "credits"], "multivalued": true}, {"name": "tfLabelMap", "description": "Mapping of a label in the input annotations to a new label. Must be formatted as IN_LABEL:OUT_LABEL (with a colon). To pass multiple mappings, use this parameter multiple times. When no remap is used, all the input labels are passed as is, including any negative labels (`-`). However, when at least one label is remapped, all the other \"unset\" labels are remapped to the negative label (`-`). Only applies when `useStitcher=true`", "type": "map", "default": ["B:bars", "S:slate", "I:chyron", "N:chyron", "Y:chyron", "C:credits", "R:credits", "W:other_opening", "L:other_opening", "O:other_opening", "M:other_opening", "E:other_text", "K:other_text", "G:other_text", "T:other_text", "F:other_text"], "multivalued": true}, {"name": "pretty", "description": "The JSON body of the HTTP response will be re-formatted with 2-space indentation", "type": "boolean", "default": false, "multivalued": false}, {"name": "runningTime", "description": "The running time of the app will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}, {"name": "hwFetch", "description": "The hardware information (architecture, GPU and vRAM) will be recorded in the view metadata", "type": "boolean", "default": false, "multivalued": false}]}]